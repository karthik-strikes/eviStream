"""
Cognitive Form Decomposition

LLM-powered intelligent decomposition of multi-field forms into atomic DSPy signatures
based on cognitive behavior analysis and dependencies.

This module uses prompt engineering to guide an LLM through analyzing form fields 
and grouping them optimally. The pipeline is auto-generated by code.
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List

from dotenv import load_dotenv

from .models import Stage1Output, Signature
from utils.lm_config import get_langchain_model

load_dotenv()

# Get logger (let application configure logging)
logger = logging.getLogger(__name__)

# Constants
MAX_DECOMPOSITION_TOKENS = 40000
LLM_TIMEOUT_SECONDS = 600


def _prepare_stage1_prompt(form_data: Dict[str, Any], feedback: Optional[str] = None) -> str:
    """
    Load and prepare Decomposition prompt for field grouping.

    Args:
        form_data: Form specification with name, description, fields
        feedback: Optional validation feedback from previous attempts

    Returns:
        Prepared prompt string with form data and feedback injected
    """
    stage1_template = Path(__file__).parent / "prompts" / \
        "decompose_form.md"
    stage1_prompt = stage1_template.read_text(encoding="utf-8")
    stage1_prompt = stage1_prompt.replace(
        "[[FORM_DATA_JSON]]", json.dumps(form_data, indent=2)
    )

    if feedback:
        stage1_prompt += f"""

═══════════════════════════════════════════════════════════════════════════════
VALIDATION FEEDBACK FROM PREVIOUS ATTEMPT
═══════════════════════════════════════════════════════════════════════════════

{feedback}

Please fix the issues mentioned above. Ensure all fields are covered exactly once.
"""

    return stage1_prompt


def _execute_stage1(model, stage1_prompt: str, max_retries: int = 3) -> Stage1Output:
    """
    Execute Decomposition: Generate field groupings with structured output.
    Includes internal retry logic for robustness.

    Args:
        model: LangChain model instance
        stage1_prompt: Prepared prompt for Decomposition
        max_retries: Maximum retry attempts for this stage

    Returns:
        Stage1Output Pydantic model with signatures

    Raises:
        ValueError: If Decomposition generation fails after all retries
    """
    logger.info("Decomposition: Generating field groupings...")

    last_error = None
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                logger.info(f"  Retry attempt {attempt + 1}/{max_retries}...")

            structured_model = model.with_structured_output(Stage1Output)

            stage1_result = structured_model.invoke(stage1_prompt)

            if stage1_result is None:
                raise ValueError(
                    "Decomposition returned None - model may not support structured output"
                )

            logger.info(
                f"Decomposition complete: {len(stage1_result.signatures)} signatures generated"
            )

            return stage1_result

        except Exception as e:
            last_error = e
            logger.warning(
                f"Decomposition attempt {attempt + 1}/{max_retries} failed: {e}"
            )

            if attempt < max_retries - 1:
                logger.info(f"  Retrying Decomposition...")
            else:
                logger.error(
                    f"Decomposition failed after {max_retries} attempts")

    raise ValueError(
        f"Decomposition (Field Grouping) generation failed after {max_retries} attempts: {last_error}"
    )


def _build_field_coverage(signatures: List[Signature]) -> Dict[str, str]:
    """
    Build field_name -> signature_name mapping from signatures.

    Args:
        signatures: List of Signature objects

    Returns:
        Dict mapping field_name to signature_name
    """
    logger.info("Building field coverage map...")
    field_coverage = {}

    for sig in signatures:
        for field_name in sig.field_names:
            field_coverage[field_name] = sig.name

    logger.info(f"  Field coverage: {len(field_coverage)} fields mapped")
    return field_coverage


def _enrich_signatures_with_metadata(
    signatures: List[Signature],
    form_data: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Enrich LLM-generated signatures with canonical field metadata from form_data.
    """
    logger.info("Enriching signatures with metadata from form_data...")

    # Build lookup map by field_name
    form_fields_map = {
        field["field_name"]: field
        for field in form_data.get("fields", [])
    }

    enriched_signatures = []

    for sig in signatures:
        enriched_sig = {
            "name": sig.name,
            "fields": {},
            "depends_on": sig.depends_on or []
        }

        for field_name in sig.field_names:
            original_field = form_fields_map.get(field_name)

            if not original_field:
                logger.warning(
                    f"Field '{field_name}' not found in form_data, skipping"
                )
                continue

            # Canonical field enrichment
            enriched_field = {
                "field_name": original_field["field_name"],
                "field_type": original_field["field_type"],
                "field_control_type": original_field["field_control_type"],
                "field_description": original_field["field_description"],
            }

            # Optional attributes (copied only if present)
            for attr in ["options", "example", "extraction_hints"]:
                if attr in original_field:
                    enriched_field[attr] = original_field[attr]

            enriched_sig["fields"][field_name] = enriched_field

        enriched_signatures.append(enriched_sig)

    logger.info(f"Enriched {len(enriched_signatures)} signatures")
    return enriched_signatures


def _auto_generate_pipeline(signatures: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Automatically generate optimized pipeline stages from signature dependencies.

    Groups signatures that can run in parallel and correctly handles dependency chains.

    Args:
        signatures: List of enriched signature dicts

    Returns:
        List of pipeline stages with execution order
    """
    logger.info("Auto-generating pipeline from dependencies...")

    # Separate independent and dependent signatures
    independent = [sig for sig in signatures if not sig["depends_on"]]
    dependent = [sig for sig in signatures if sig["depends_on"]]

    pipeline = []
    stage_outputs = {}  # Track which stage produces which fields

    # Stage 1: Independent signatures (run in parallel)
    if independent:
        stage1_fields = []
        for sig in independent:
            stage1_fields.extend(sig["fields"].keys())

        pipeline.append({
            "stage": 1,
            "signatures": [sig["name"] for sig in independent],
            "execution": "parallel",
            "provides_fields": stage1_fields,
            "requires_fields": []
        })

        # Track stage 1 outputs
        stage_outputs[1] = set(stage1_fields)

    # Process dependent signatures in waves
    # Group signatures that have the same dependency "depth"
    remaining = dependent.copy()
    current_stage = 2

    while remaining:
        # Find signatures that can run now (all dependencies satisfied)
        ready_signatures = []

        for sig in remaining:
            deps_set = set(sig["depends_on"])

            # Check if all dependencies are satisfied by previous stages
            all_deps_satisfied = True
            for dep_field in deps_set:
                found = False
                for stage_num, fields in stage_outputs.items():
                    if dep_field in fields:
                        found = True
                        break
                if not found:
                    all_deps_satisfied = False
                    break

            if all_deps_satisfied:
                ready_signatures.append(sig)

        if not ready_signatures:
            # Safety check - should not happen with valid dependencies
            logger.error(
                f"Cannot satisfy dependencies for remaining signatures: {[s['name'] for s in remaining]}")
            break

        # Determine execution mode and waits_for_stage
        if len(ready_signatures) == 1:
            execution = "sequential"
        else:
            # Check if they can truly run in parallel (don't depend on each other)
            can_parallel = True
            ready_fields = set()
            for sig in ready_signatures:
                ready_fields.update(sig["fields"].keys())

            for sig in ready_signatures:
                deps_set = set(sig["depends_on"])
                if deps_set.intersection(ready_fields):
                    # This signature depends on another in this group
                    can_parallel = False
                    break

            execution = "parallel" if can_parallel else "sequential"

        # Find the latest stage this group depends on
        max_dependency_stage = 0
        all_required_fields = set()

        for sig in ready_signatures:
            all_required_fields.update(sig["depends_on"])

        for dep_field in all_required_fields:
            for stage_num, fields in stage_outputs.items():
                if dep_field in fields:
                    max_dependency_stage = max(max_dependency_stage, stage_num)

        # Create stage
        stage_fields = []
        for sig in ready_signatures:
            stage_fields.extend(sig["fields"].keys())

        pipeline.append({
            "stage": current_stage,
            "signatures": [sig["name"] for sig in ready_signatures],
            "execution": execution,
            "provides_fields": stage_fields,
            "requires_fields": list(all_required_fields),
            "waits_for_stage": max_dependency_stage
        })

        # Track this stage's outputs
        stage_outputs[current_stage] = set(stage_fields)

        # Remove processed signatures
        for sig in ready_signatures:
            remaining.remove(sig)

        current_stage += 1

    logger.info(f"  Generated {len(pipeline)} pipeline stages")

    # Log pipeline structure for debugging
    for stage in pipeline:
        logger.info(
            f"    Stage {stage['stage']}: {stage['signatures']} ({stage['execution']})")

    return pipeline


def decompose_form(
    form_data: Dict[str, Any],
    model_name: str = "google_genai:gemini-2.0-flash-exp",
    feedback: Optional[str] = None,
    max_retries: int = 3,
) -> Dict[str, Any]:
    """
    Single-stage decomposition: LLM groups fields, code does the rest.

    Args:
        form_data: Form specification with name, description, fields
        model_name: LLM model to use (must support structured output)
        feedback: Optional validation feedback from previous attempts
        max_retries: Maximum retry attempts (default: 3)

    Returns:
        Dict containing:
        {
            "reasoning_trace": str,        # LLM's reasoning about grouping decisions
            "signatures": [...],           # Enriched with metadata from form_data
            "pipeline": [...],             # Auto-generated from dependencies
            "field_coverage": {...}        # Field name -> signature name mapping
        }
    """
    logger.info("Starting form decomposition...")
    logger.info(f"  Model: {model_name}")
    logger.info(f"  Max retries: {max_retries}")
    max_retries = 5
    model_name = "anthropic:claude-sonnet-4-5-20250929"
    model = get_langchain_model(model_name, temperature=0.2, max_tokens=20000)

    # Decomposition: LLM generates field groupings
    stage1_prompt = _prepare_stage1_prompt(form_data, feedback)
    stage1_result = _execute_stage1(
        model, stage1_prompt, max_retries=max_retries)

    logger.info("Decomposition complete")

    # Build field coverage map
    field_coverage = _build_field_coverage(stage1_result.signatures)

    # Enrich signatures with metadata from form_data
    enriched_signatures = _enrich_signatures_with_metadata(
        stage1_result.signatures,
        form_data
    )

    # Auto-generate pipeline from dependencies
    pipeline = _auto_generate_pipeline(enriched_signatures)

    # Build final decomposition (4 keys only)
    decomposition = {
        "reasoning_trace": stage1_result.reasoning_trace,
        "signatures": enriched_signatures,
        "pipeline": pipeline,
        "field_coverage": field_coverage,
    }

    # print("\n" + "="*80)
    # print("FINAL DECOMPOSITION")
    # print("="*80)
    # print(json.dumps(decomposition, indent=2, ensure_ascii=False))
    # print("="*80 + "\n")

    return decomposition


__all__ = ["decompose_form"]
